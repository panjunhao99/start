1.transformer训练的初始阶段往往不稳定，如何解决？
pre-train, 针对不稳定的原因解决

2.transformer对大数据集效果好，但医学数据集往往较小，如何利用transformer的自注意力的优势来解决这个问题？
自监督的方法，pre-train

3.医学图像尺寸较大，训练不起来怎么办，如何解决计算量过大的问题？
从局部上下手，用更多显卡，batchsize 小一点，轻量级方法，swin-t!!!,muti-scale方法

4.Transformer,MLP,CNN 三者优势是什么，怎么把他们结合起来？
小数据CNN可能更好一点，结合方式无非就是串联和并联，结合是一个大趋势
可以思考，如何把transformer用在medial的特性上。

5.Transformer强大的建模能力是否能在未来大放异彩？今后的研究思路有哪些？
golbal的能力

6.Transformer在计算机视觉中的限制和挑战是什么？哪种结构设计更加灵活？
